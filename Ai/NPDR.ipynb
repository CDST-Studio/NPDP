{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NPDR.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONpilFzfOclSnKf0M94OHw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mKJOzzyKuOOa"},"source":["# Reference\n","\n","*   [YORO v3](https://deep-learning-study.tistory.com/411)\n","*   [Dataset](https://aihub.or.kr/aidata/27727)\n","*   [IoU](https://ballentain.tistory.com/12)\n","*   [Objectness(=Confidence threshold)](https://mickael-k.tistory.com/141)\n","\n","# Data source\n","\n","*   [cfg File](https://github.com/pjreddie/darknet)\n","*   [weights File](https://pjreddie.com/media/files/yolov3.weights)\n","\n","# Issue\n","\n","*   [Modify yolo_v3.cfg, width and height: 608 → 416](https://discuss.pytorch.org/t/shape-1-255-3025-is-invalid-for-input-of-size-689520/37603/11)\n","*   [링크 텍스트](https://)\n","\n","# Function info\n","\n","*   [squeeze, unsqueeze](https://sanghyu.tistory.com/86)\n"]},{"cell_type":"markdown","metadata":{"id":"yB-LsihlzaZm"},"source":["# Google Drive Mount"]},{"cell_type":"code","metadata":{"id":"QbSyApmNzbkl"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","%cd /content/gdrive/MyDrive/DeepLearning/Project/HawkEye\n","!ls -al"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykKHgUyaqQ-g"},"source":["# Import Package"]},{"cell_type":"code","metadata":{"id":"EgiQgJYBONWg"},"source":["from __future__ import division\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7AM_UoL0A8aZ"},"source":["# Functions\n","\n","*The neural network configuration brought up by the author of [the paper](https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg)*"]},{"cell_type":"code","metadata":{"id":"q9BtaePcA8hs"},"source":["# Configuration file read\n","def parse_cfg(cfgFile) :\n","    f = open(cfgFile, 'r')\n","    lines = f.read().split('\\n')\n","\n","    lines = [x for x in lines if len(x) > 0] # empty delete\n","    lines = [x for x in lines if x[0] != '#'] # comment delete\n","    lines = [x.rstrip().lstrip() for x in lines] # space delete\n","\n","    block = {}\n","    blocks = []\n","\n","    for line in lines :\n","        if line[0] == '[' : # start new block\n","            if len(block) != 0 : # not empty\n","                blocks.append(block)\n","                block = {}\n","            \n","            block['type'] = line[1 : -1].rstrip()\n","        else :\n","            key, value = line.split('=')\n","            block[key.rstrip()] = value.lstrip()\n","        \n","    blocks.append(block)\n","\n","    return blocks\n","\n","# Build a Pytorch module for blocks of a configuration file.\n","def create_modules(blocks) :\n","    net_info = blocks[0] # save input and pre-processing data\n","    module_list = nn.ModuleList()\n","\n","    prev_filters = 3 # RGB(=3) filter\n","    output_filters = [] # each block's filters count append\n","\n","    for idx, x in enumerate(blocks[1:]) :\n","        module = nn.Sequential() # Use Sequential layer\n","\n","        if x['type'] == 'convolutional' : # block type confirm\n","            # get layer's infomation\n","            activation = x[\"activation\"]\n","            try :\n","                batch_normalize = int(x['batch_normalize'])\n","                bias = False\n","            except :\n","                batch_normalize = 0\n","                bias = True\n","\n","            filters = int(x['filters'])\n","            padding = int(x['pad'])\n","            kernel_size = int(x['size'])\n","            stride = int(x['stride'])\n","\n","            if padding : pad = (kernel_size - 1) // 2\n","            else : pad = 0\n","\n","            # add layer(2D convolutional layer)\n","            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=bias)\n","            module.add_module('conv_{0}'.format(idx), conv)\n","\n","            # add layer(Batch norm layer)\n","            if batch_normalize :\n","                bn = nn.BatchNorm2d(filters)\n","                module.add_module('batch_norm_{0}'.format(idx), bn)\n","\n","            # check activation, Leaky ReLU or Linear\n","            if activation == 'leaky' :\n","                activn = nn.LeakyReLU(0.1, inplace=True)\n","                module.add_module('leaky_{0}'.format(idx), activn)\n","\n","        elif x['type'] == 'upsample' : # block type == upsampling\n","            stride = int(x['stride'])\n","            upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n","            module.add_module('upsample_{}'.format(idx), upsample)\n","\n","        elif x['type'] == 'route' : # block type == route\n","            x['layers'] = x['layers'].split(',') # get layer's parameter and split\n","            start = int(x['layers'][0])\n","\n","            try : end = int(x['layers'][1])\n","            except : end = 0\n","\n","            # positive\n","            if start > 0 : start = start - idx\n","            if end > 0 : end = end - idx\n","            route = EmptyLayer()\n","            module.add_module('route_{0}'.format(idx), route)\n","            \n","            # negative\n","            if end < 0 : filters = output_filters[idx + start] + output_filters[idx + end]\n","            else : filters = output_filters[idx + start]\n","\n","        elif x['type'] == 'shortcut' : # block type == skip connection(shortcut)\n","            shortcut = EmptyLayer()\n","            module.add_module('shortcut_{}'.format(idx), shortcut)\n","\n","        elif x['type'] == 'yolo' : # block type == YOLO\n","            mask = x['mask'].split(',')\n","            mask = [int(x) for x in mask]\n","\n","            anchors = x['anchors'].split(',')\n","            anchors = [int(a) for a in anchors]\n","            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors), 2)]\n","            anchors = [anchors[i] for i in mask]\n","            detection = DetectionLayer(anchors)\n","            module.add_module('Detection_{}'.format(idx), detection)\n","\n","        module_list.append(module)\n","        prev_filters = filters\n","        output_filters.append(filters)\n","\n","    return (net_info, module_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2aUFnjhB3J_1"},"source":["***Util***"]},{"cell_type":"code","metadata":{"id":"KqT4j50d3O4L"},"source":["# Detection feature map convert 2-D tensor\n","def prediction_transform(prediction, inp_dim, anchors, num_classes, CUDA=True) :\n","    batch_size = prediction.size(0)\n","    stride = inp_dim // prediction.size(2)\n","    grid_size = inp_dim // stride\n","    bbox_attrs = 5 + num_classes\n","    num_anchors = len(anchors)\n","\n","    prediction = prediction.view(batch_size, bbox_attrs * num_anchors, grid_size * grid_size)\n","    prediction = prediction.transpose(1, 2).contiguous()\n","    prediction = prediction.view(batch_size, grid_size * grid_size * num_anchors, bbox_attrs)\n","\n","    # anchors dimension > detection map dimension so, divide anchors by detection feature map's stride\n","    anchors = [(a[0] / stride, a[1] / stride) for a in anchors]\n","\n","    # sigmoid the center x,y coordinates and object confidence.\n","    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n","    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n","    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n","\n","    # add center offset\n","    grid = np.arange(grid_size)\n","    a, b = np.meshgrid(grid, grid)\n","\n","    x_offset = torch.FloatTensor(a).view(-1, 1)\n","    y_offset = torch.FloatTensor(b).view(-1, 1)\n","\n","    if CUDA :\n","        x_offset = torch.FloatTensor(a).view(-1, 1).cuda()\n","        y_offset = torch.FloatTensor(b).view(-1, 1).cuda()\n","\n","    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1, num_anchors).view(-1, 2).unsqueeze(0)\n","    prediction[:,:,:2] += x_y_offset\n","\n","    # apply anchors to the dimension of the bounding box.\n","    anchors = torch.FloatTensor(anchors) # convert width, height by log space\n","    if CUDA : anchors = torch.FloatTensor(anchors).cuda()\n","\n","    anchors = anchors.repeat(grid_size * grid_size, 1).unsqueeze(0)\n","    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4]) * anchors\n","\n","    # apply class score to sigmoid activation\n","    prediction[:, :, 5: 5 + num_classes] = torch.sigmoid((prediction[:, :, 5: 5 + num_classes]))\n","\n","    # resize detection map to input img size\n","    prediction[:, :, :4] *= stride\n","\n","    return prediction\n","\n","def write_results(prediction, confidence, num_classes, mns_conf=0.4) :\n","    \"\"\"\n","    < Explanation of terms >\n","\n","    bounding box = A box of the minimum size that can contain all the images.\n","    threshold = It is similar processing of the image.\n","    true detections = The scope that needs to be recognized.\n","    confidence thresholding(=Objectness) = It determines the possibility and accuracy of whether the Box has an object.\n","    \"\"\"\n","    # set the property of the bounding box with an objectness score lower than the threshold to zero.\n","    conf_mask = (prediction[:, :, 4] > confidence).float().unsqueeze(2)\n","    prediction = prediction * conf_mask\n","\n","    # change the center point to the upper left and lower right corner coordinates as follows. It is easy to calculate the IoU(Reference) of two boxes using the coordinates of the diagonal edge pair of boxes.\n","    box_corner = prediction.new(prediction.shape)\n","    box_corner[:, :, 0] = prediction[:, :, 0] - (prediction[:, :, 2] / 2)\n","    box_corner[:, :, 1] = prediction[:, :, 1] - (prediction[:, :, 3] / 2)\n","    box_corner[:, :, 2] = prediction[:, :, 0] + (prediction[:, :, 2] / 2)\n","    box_corner[:, :, 3] = prediction[:, :, 1] + (prediction[:, :, 3] / 2)\n","    prediction[:, :, 4] = box_corner[:, :, 4]\n","\n","    for ind in range(batch_size) :\n","        image_pred = prediction[ind]\n","\n","        # delete everything except for the class score with the highest value.\n","        max_conf, max_conf_score = torch.max(image_pred[:, 5:5 + num_classes], 1)\n","        max_conf = max_conf.float().unsqueeze(1)\n","        max_conf_score = max_conf_score.float().unsqueeze(1)\n","        seq = (image_pred[:, :5], max_conf, max_conf_score)\n","        image_pred = torch.cat(seq, 1)\n","\n","        # remove setting bounding box rows to 0 with object confidence lower than threshold.\n","        non_zero_ind = torch.nonzero(image_pred[:, 4])\n","        try : image_pred_ = image_pred[non_zero_ind.squeeze(), :].view(-1, 7)\n","        except : continue # no detection\n","\n","        # For PyTorch 0.4 compatibility\n","        # Since the above code with not raise exception for no detection as scalars are supported in PyTorch 0.4\n","        if image_pred_.shaep[0] == 0 : continue\n","\n","        # get various classes\n","        img_classes = unique(image_pred_[:, -1])\n","\n","        for cls in img_classes :\n","            # get each class's detections\n","            cls_mask = image_pred_ * (image_pred_[:, -1] == cls).float().unsqueeze(1)\n","            class_mask_ind = torch.nonzero(cls_mask[:, -2]).squeeze()\n","            image_pred_[class_mask_ind].view(-1, 7)\n","\n","            # sort in the order of detections with the highest objectness. confidence is at the top.\n","            conf_sort_index = torch.sort(image_pred_class[:, 4], descending=True)[1]\n","            image_pred_class = image_pred_class[conf_sort_index]\n","            idx = image_pred_class.size(0) # detections count\n","\n","            # excute NMS\n","            for i in range(idx) :\n","                # get IoU about all of each box\n","                try : ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i + 1, :])\n","                except ValueError : break\n","                except IndexError : break\n","\n","                # if IoU > threshhold, detections = 0\n","                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n","                image_pred_class[i + 1:] *= iou_mask\n","\n","                # delete non-zero\n","                non_zero_ind = torch.nonzero(image_pred_class[:, 4]).squeeze()\n","                image_pred_class = image_pred_class[non_zero_ind].view(-1, 7)\n","\n","                # IoU of box with i index and Bounding boxes with index greater than i.\n","                ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i + 1, :])\n","\n","                # if IoU > threshhold, detections = 0\n","                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n","                image_pred_class[i + 1:] *= iou_mask\n","\n","                # delete non-zero, The number of bounding boxes is removed by image_pred_class. This means that if any value has been removed by image_pred_class, it cannot have identifications.\n","                non_zero_ind = torch.nonzero(image_pred_class[:, 4]).squeeze()\n","                image_pred_class = image_pred_class[non_zero_ind].view(-1, 7)\n","\n","                # repeat batch_id as much as the detections of the class in the image.\n","                batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n","                seq = batch, image_pred_class\n","\n","                if not write :\n","                    output = torch.cat(seq, 1)\n","                    write = True\n","                else :\n","                    out = torch.cat(seq, 1)\n","                    output = torch.cat((output, out))\n","\n","        # check if output is initialized. Returns 0 if any detection was not detected in any image in the batch.\n","        try : return output # Each prediction has a predicted value in the form of a sensor grouped together.\n","        except : return 0\n","\n","# Get image class\n","def unique(tensor) :\n","    tensor_np = tensor.cpu().numpy()\n","    unique_np = np.unique(tensor_np)\n","    unique_tensor = torch.from_numpy(unique_np)\n","\n","    tensor_res = tensor.new(unique_tensor.shape)\n","    tensor_res.copy_(unique_tensor)\n","\n","    return tensor_res\n","\n","# Compute IoU\n","def bbox_iou(box1, box2) :\n","    # get bounding boxes's coordinates\n","    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box[:, 1], box1[: ,2], box1[:, 3]\n","    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n","\n","    # get intersection intercept coordinates\n","    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n","    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n","    inter_rect_x2 = torch.max(b1_x2, b2_x2)\n","    inter_rect_y2 = torch.max(b1_y2, b2_y2)\n","\n","    # area intersection\n","    inter_area = torch.clamp(inter_rect_x2 - inter.rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n","\n","    # area union\n","    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n","    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n","\n","    # compute IoU score\n","    iou = inter_area / (b1_area + b2_area - inter_area)\n","\n","    return iou"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7-HEEQnU3PnG"},"source":["***Test***"]},{"cell_type":"code","metadata":{"id":"2Rkvu0Jj3PsW"},"source":["def get_test_input() :\n","    img = cv2.imread('./Dataset/dog-cycle-car.png')\n","    img = cv2.resize(img, (416, 416)) # resize to input size\n","    img_ = img[:, :, ::-1].transpose((2, 0, 1)) # BGR - > RGB, HxWxC -> CxHxW\n","    img_ = img_[np.newaxis, :, :, :] / 255.0 # add 0 chanel for batch and normalize\n","    img_ = torch.from_numpy(img_).float()\n","    img_ = Variable(img_)\n","\n","    return img_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDb1h6m4NX7Q"},"source":["# Class"]},{"cell_type":"code","metadata":{"id":"xWhxRU_BNYA_"},"source":["# Dummy layer for object's (of nn.Module) direct connection on forward function for simple processing\n","class EmptyLayer(nn.Module) :\n","    def __init__(self) : \n","        super(EmptyLayer, self).__init__()\n","\n","# Layer save anchors for detection about bounding box\n","class DetectionLayer(nn.Module) :\n","    def __init__(self, anchors) :\n","        super(DetectionLayer, self).__init__()\n","\n","        self.anchors = anchors\n","\n","class Darknet(nn.Module) :\n","    def __init__(self, cfgFile) :\n","        super(Darknet, self).__init__()\n","\n","        self.blocks = parse_cfg(cfgFile)\n","        self.net_info, self.module_list = create_modules(self.blocks)\n","\n","    \"\"\"\n","    Purpose: The first is to calculate the output, and the second is to convert the output detection feature map into an easy-to-process method.\n","    Converting to detection maps across various scales leads to connection. Otherwise, it is impossible because it is at different dimensions.\n","    \"\"\"\n","    def forward(self, x, CUDA) :\n","        modules = self.blocks[1:]\n","        outputs = {} # all layer's output\n","\n","        write = 0 # 0 == outputs is not initate\n","        for i, module in enumerate(modules) :\n","            module_type = (module['type'])\n","\n","            if module_type == 'convolutional' or module_type == 'upsample' : \n","                x = self.module_list[i](x) # convolutional or upsample layer = simple forward\n","                \n","            elif module_type == 'route' :\n","                layers = module['layers']\n","                layers = [int(a) for a in layers]\n","\n","                if layers[0] > 0 : layers[0] = layers[0] - i\n","\n","                if len(layers) == 1 : x = outputs[i + layers[0]]\n","                else :\n","                    if layers[1] > 0 : layers[1] = layers[1] - i\n","\n","                    map1 = outputs[i + layers[0]]\n","                    map2 = outputs[i + layers[1]]\n","\n","                    x = torch.cat((map1, map2), 1) # two feature map connect\n","\n","            elif module_type == 'shortcut' :\n","                from_ = int(module['from'])\n","                x = outputs[i - 1] + outputs[i + from_]\n","\n","            elif module_type == 'yolo' :\n","                anchors = self.module_list[i][0].anchors\n","\n","                inp_dim = int(self.net_info['height']) # get input dimensions\n","                num_classes = int(module['classes']) # get class count\n","\n","                x = x.data\n","                x = prediction_transform(x, inp_dim, anchors, num_classes, CUDA)\n","                if not write :\n","                    detections = x\n","                    write = 1\n","                else :\n","                    detections = torch.cat((detections, x), 1)\n","\n","            outputs[i] = x\n","\n","        return detections\n","\n","    # Load weights\n","    def load_weights(self, weightFile) :\n","        fp = open(weightFile, 'rb') # open weights file\n","        \n","        \"\"\"\n","        1~5 value(first 160 bytes) is header infomation\n","        - 1. Magor version number\n","        - 2. Minor version number\n","        - 3. Subversion number\n","        - 4, 5. An image learned by neural networks in training\n","        \"\"\"\n","        header = np.fromfile(fp, dtype=np.int32, count=5)\n","        \n","        self.header = torch.from_numpy(header)\n","        self.seen = self.header[3]\n","\n","        # rest bits represent weights in order. weight is saved by float32 format\n","        weights = np.fromfile(fp, dtype=np.float32)\n","        \n","        ptr = 0 # \n","        for i in range(len(self.module_list)) :\n","            module_type = self.blocks[i + 1]['type']\n","\n","            if module_type == 'convolutional' :\n","                model = self.module_list[i]\n","\n","                # load weight\n","                try : batch_normalize = int(self.blocks[i + 1]['batch_normalize'])\n","                except : batch_normalize = 0\n","\n","                conv = model[0]\n","\n","                if batch_normalize :\n","                    bn = model[1]\n","\n","                    # get batch norm layer's weight count\n","                    num_bn_biases = bn.bias.numel()\n","\n","                    # load weights\n","                    bn_biases = torch.from_numpy(weights[ptr : ptr + num_bn_biases])\n","                    ptr += num_bn_biases\n","\n","                    bn_weights = torch.from_numpy(weights[ptr : ptr + num_bn_biases])\n","                    ptr += num_bn_biases\n","\n","                    bn_running_mean = torch.from_numpy(weights[ptr : ptr + num_bn_biases])\n","                    ptr += num_bn_biases\n","\n","                    bn_running_var = torch.from_numpy(weights[ptr : ptr + num_bn_biases])\n","                    ptr += num_bn_biases\n","\n","                    # convert the imported weights to the dimension of the model weights.\n","                    bn_biases = bn_biases.view_as(bn.bias.data)\n","                    bn_weights = bn_weights.view_as(bn_weights.data)\n","                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n","                    bn_running_var = bn_running_var.view_as(bn.running_var)\n","\n","                    # copy data to model\n","                    bn.bias.data.copy_(bn_biases)\n","                    bn.weight.data.copy_(bn_weights)\n","                    bn.running_mean.copy_(bn_running_mean)\n","                    bn.running_var.copy_(bn_running_var)\n","\n","                else :\n","                    # bring up the biases of the convolutional layer.\n","                    num_biases = conv.bias.numel()\n","\n","                    conv_biases = torch.from_numpy(weights[ptr : ptr + num_biases])\n","                    ptr += num_biases\n","\n","                    # reshape loaded weights\n","                    conv_biases = conv_biases.view_as(conv.bias.data)\n","\n","                    # copy data\n","                    conv.bias.data.copy_(conv_biases)\n","\n","                # load weights about Convolutional layer\n","                num_weights = conv.weight.numel()\n","\n","                # weights same to up that\n","                conv_weights = torch.from_numpy(weights[ptr : ptr + num_weights])\n","                ptr += num_weights\n","\n","                conv_weights = conv_weights.view_as(conv.weight.data)\n","                conv.weight.data.copy_(conv_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJzGbcEwvu5u"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"aCzwnvWFvvBu"},"source":["model = Darknet(\"./Dataset/yolo_v3.cfg\")\n","model.load_weights('./Dataset/yolo_v3.weights')\n","inp = get_test_input()\n","\n","print(\"CUDA is available? \",torch.cuda.is_available())\n","pred = model(inp, torch.cuda.is_available())\n","\n","print(pred)"],"execution_count":null,"outputs":[]}]}